{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, train_test_split, cross_val_predict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "df = pd.read_parquet('processed_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cols = ['age_ind', 'district_y', 'crash_hour_ind', 'posted_speed_limit_ind', 'street_direction_ind_S', 'street_direction_ind_N',\n",
    " 'lighting_condition_ind_DAYLIGHT','crash_day_of_week_ind', 'target']\n",
    "\n",
    "df = df[input_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y\n",
    "X = df.drop(columns=['target']).to_numpy() \n",
    "y = df['target'].to_numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    11937\n",
       "0     7411\n",
       "2     3265\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes can be extended to handle multi-class classification problems. In this context, it calculates the probability of each class given the features and selects the class with the highest probability as the predicted class.\n",
    "\n",
    "Logistic Regression can be extended to handle multi-class classification tasks. One common approach is the one-vs-rest (OvR) strategy, where separate binary classifiers are trained for each class. Each classifier is trained to distinguish between one class and the rest. Alternatively, the one-vs-one (OvO) strategy trains a binary classifier for each pair of classes.\n",
    "\n",
    "SVM: SVM can be adapted to handle multi-class classification using either the one-vs-one (OvO) or one-vs-rest (OvR) strategy. In OvO, a binary classifier is trained for each pair of classes, and the class with the most votes is chosen. In OvR, separate binary classifiers are trained for each class, where each classifier distinguishes between one class and the rest.\n",
    "\n",
    "Random Forest: Ensemble learning method that combines the strengths of decision trees with randomization to achieve high predictive accuracy and generalization performance.\n",
    "\n",
    "Gradient Boosting: Builds an ensemble of weak learners, optimizing them using gradient descent to minimize a loss function and achieve strong predictive performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class 0: 0.1753\n",
      "Accuracy for class 1: 0.9079\n",
      "Accuracy for class 2: 0.0021\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def naive_bayes_accuracy_per_class(X, y, n_splits=5, n_repeats=2):\n",
    "    nb_classifier = GaussianNB()\n",
    "\n",
    "    # Perform cross-validation and get predicted labels for each sample\n",
    "    y_pred = cross_val_predict(nb_classifier, X, y, cv=n_splits, n_jobs=-1)\n",
    "\n",
    "    # accuracy for each class\n",
    "    accuracy_per_class = []\n",
    "    for class_label in np.unique(y):\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        class_accuracy = accuracy_score(y[class_indices], y_pred[class_indices])\n",
    "        accuracy_per_class.append(class_accuracy)\n",
    "\n",
    "    return accuracy_per_class\n",
    "\n",
    "# Example:\n",
    "accuracies = naive_bayes_accuracy_per_class(X, y)\n",
    "for i, acc in enumerate(accuracies):\n",
    "    print(f\"Accuracy for class {i}: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/chicago/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/envs/chicago/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/envs/chicago/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "/opt/anaconda3/envs/chicago/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class 0: 0.0734\n",
      "Accuracy for class 1: 0.9778\n",
      "Accuracy for class 2: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/chicago/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic_regression_accuracy_per_class(X, y, n_splits=5, n_repeats=2):\n",
    "    lr_classifier = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "    # cross-validation and get predicted labels for each sample\n",
    "    y_pred = cross_val_predict(lr_classifier, X, y, cv=n_splits, n_jobs=-1)\n",
    "\n",
    "    # accuracy for each class\n",
    "    accuracy_per_class = []\n",
    "    for class_label in np.unique(y):\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        class_accuracy = accuracy_score(y[class_indices], y_pred[class_indices])\n",
    "        accuracy_per_class.append(class_accuracy)\n",
    "\n",
    "    return accuracy_per_class\n",
    "\n",
    "# Example:\n",
    "accuracies = logistic_regression_accuracy_per_class(X, y)\n",
    "for i, acc in enumerate(accuracies):\n",
    "    print(f\"Accuracy for class {i}: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class 0: 0.2154\n",
      "Accuracy for class 1: 0.9261\n",
      "Accuracy for class 2: 0.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def svm_accuracy_per_class(X, y, n_splits=5, n_repeats=2):\n",
    "    svm_classifier = SVC()\n",
    "\n",
    "    # cross-validation and get predicted labels for each sample\n",
    "    y_pred = cross_val_predict(svm_classifier, X, y, cv=n_splits, n_jobs=-1)\n",
    "\n",
    "    # accuracy for each class\n",
    "    accuracy_per_class = []\n",
    "    for class_label in np.unique(y):\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        class_accuracy = accuracy_score(y[class_indices], y_pred[class_indices])\n",
    "        accuracy_per_class.append(class_accuracy)\n",
    "\n",
    "    return accuracy_per_class\n",
    "\n",
    "# Example:\n",
    "accuracies = svm_accuracy_per_class(X, y)\n",
    "for i, acc in enumerate(accuracies):\n",
    "    print(f\"Accuracy for class {i}: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class 0: 0.3330\n",
      "Accuracy for class 1: 0.6923\n",
      "Accuracy for class 2: 0.0833\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def random_forest_accuracy_per_class(X, y, n_splits=5, n_repeats=2):\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "\n",
    "    # cross-validation and get predicted labels for each sample\n",
    "    y_pred = cross_val_predict(rf_classifier, X, y, cv=n_splits, n_jobs=-1)\n",
    "\n",
    "    # accuracy for each class\n",
    "    accuracy_per_class = []\n",
    "    for class_label in np.unique(y):\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        class_accuracy = accuracy_score(y[class_indices], y_pred[class_indices])\n",
    "        accuracy_per_class.append(class_accuracy)\n",
    "\n",
    "    return accuracy_per_class\n",
    "\n",
    "# Example:\n",
    "accuracies = random_forest_accuracy_per_class(X, y)\n",
    "for i, acc in enumerate(accuracies):\n",
    "    print(f\"Accuracy for class {i}: {acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for class 0: 0.2086\n",
      "Accuracy for class 1: 0.9332\n",
      "Accuracy for class 2: 0.0089\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def gradient_boosting_accuracy_per_class(X, y, n_splits=5, n_repeats=2):\n",
    "    gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "    # cross-validation and predicted class for each sample\n",
    "    y_pred = cross_val_predict(gb_classifier, X, y, cv=n_splits, n_jobs=-1)\n",
    "\n",
    "    # accuracy for each class\n",
    "    accuracy_per_class = []\n",
    "    for class_label in np.unique(y):\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        class_accuracy = accuracy_score(y[class_indices], y_pred[class_indices])\n",
    "        accuracy_per_class.append(class_accuracy)\n",
    "\n",
    "    return accuracy_per_class\n",
    "\n",
    "# Example:\n",
    "accuracies = gradient_boosting_accuracy_per_class(X, y)\n",
    "for i, acc in enumerate(accuracies):\n",
    "    print(f\"Accuracy for class {i}: {acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chicago",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
