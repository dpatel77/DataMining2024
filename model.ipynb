{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits \n",
    "\n",
    "X = df[['target']]\n",
    "y = df.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes can be extended to handle multi-class classification problems. In this context, it calculates the probability of each class given the features and selects the class with the highest probability as the predicted class.\n",
    "\n",
    "Logistic Regression can be extended to handle multi-class classification tasks. One common approach is the one-vs-rest (OvR) strategy, where separate binary classifiers are trained for each class. Each classifier is trained to distinguish between one class and the rest. Alternatively, the one-vs-one (OvO) strategy trains a binary classifier for each pair of classes.\n",
    "\n",
    "SVM: SVM can be adapted to handle multi-class classification using either the one-vs-one (OvO) or one-vs-rest (OvR) strategy. In OvO, a binary classifier is trained for each pair of classes, and the class with the most votes is chosen. In OvR, separate binary classifiers are trained for each class, where each classifier distinguishes between one class and the rest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "\n",
    "naive_bayes_model = GaussianNB()\n",
    "logistic_model = LogisticRegression(max_iter=1000)\n",
    "svm_model = LinearSVC(max_iter=10000)  # Increase max_iter for convergence\n",
    "random_forest_model = RandomForestClassifier()\n",
    "gradient_boosting_model = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util function to run models \n",
    "\n",
    "def repeated_cross_validation_model(model, X, y, n_splits=5, n_repeats=3):\n",
    "    \"\"\"\n",
    "    Perform repeated cross-validation for a given multi-class model and return accuracy for each class.\n",
    "\n",
    "    Parameters:\n",
    "    - model: The multi-class machine learning model to be evaluated.\n",
    "    - X: The feature matrix.\n",
    "    - y: The target vector.\n",
    "    - n_splits: Number of folds in each repeated cross-validation.\n",
    "    - n_repeats: Number of repetitions of cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    - accuracy_per_class: Array of accuracy scores for each class.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize RepeatedStratifiedKFold\n",
    "    rskf = RepeatedStratifiedKFold(n_splits=n_splits, n_repeats=n_repeats, random_state=42)\n",
    "\n",
    "    # Perform repeated cross-validation\n",
    "    y_pred = cross_val_predict(model, X, y, cv=rskf)\n",
    "\n",
    "    # Calculate accuracy for each class\n",
    "    accuracy_per_class = accuracy_score(y, y_pred, normalize=False)\n",
    "\n",
    "    return accuracy_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run models\n",
    "naive_bayes_model_results = repeated_cross_validation_model(model=naive_bayes_model, X=X, y=y, n_splits=5, n_repeats=3)\n",
    "logistic_model_results = repeated_cross_validation_model(model=logistic_model, X=X, y=y, n_splits=5, n_repeats=3)\n",
    "svm_model_results = repeated_cross_validation_model(model=svm_model, X=X, y=y, n_splits=5, n_repeats=3)\n",
    "random_forest_model_results = repeated_cross_validation_model(model=random_forest_model, X=X, y=y, n_splits=5, n_repeats=3)\n",
    "gradient_boosting_model_results = repeated_cross_validation_model(model=gradient_boosting_model, X=X, y=y, n_splits=5, n_repeats=3)\n",
    "\n",
    "results = pd.concat([naive_bayes_model_results, logistic_model_results, svm_model_results, random_forest_model_results, gradient_boosting_model_results], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chicago",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
