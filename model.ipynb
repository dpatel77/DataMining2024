{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold, train_test_split, cross_val_predict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data \n",
    "df = pd.read_parquet('processed_data.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['age_ind', 'street_direction_ind_S', 'street_direction_ind_N'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[95], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m input_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage_ind\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistrict_y\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrash_hour_ind\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mposted_speed_limit_ind\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreet_direction_ind_S\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreet_direction_ind_N\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      2\u001b[0m  \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlighting_condition_ind_DAYLIGHT\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrash_day_of_week_ind\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_cols\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chicago/lib/python3.10/site-packages/pandas/core/frame.py:3899\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3897\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3898\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3899\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3901\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3902\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chicago/lib/python3.10/site-packages/pandas/core/indexes/base.py:6115\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6113\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6115\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6117\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6119\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/chicago/lib/python3.10/site-packages/pandas/core/indexes/base.py:6179\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6176\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6178\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6179\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['age_ind', 'street_direction_ind_S', 'street_direction_ind_N'] not in index\""
     ]
    }
   ],
   "source": [
    "input_cols = ['age_ind', 'district_y', 'crash_hour_ind', 'posted_speed_limit_ind', 'street_direction_ind_S', 'street_direction_ind_N',\n",
    " 'lighting_condition_ind_DAYLIGHT','crash_day_of_week_ind', 'target']\n",
    "\n",
    "df = df[input_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into X and y\n",
    "X = df.drop(columns=['target']).to_numpy() \n",
    "y = df['target'].to_numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "1    11937\n",
       "0     7411\n",
       "2     3265\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes can be extended to handle multi-class classification problems. In this context, it calculates the probability of each class given the features and selects the class with the highest probability as the predicted class.\n",
    "\n",
    "Logistic Regression can be extended to handle multi-class classification tasks. One common approach is the one-vs-rest (OvR) strategy, where separate binary classifiers are trained for each class. Each classifier is trained to distinguish between one class and the rest. Alternatively, the one-vs-one (OvO) strategy trains a binary classifier for each pair of classes.\n",
    "\n",
    "SVM: SVM can be adapted to handle multi-class classification using either the one-vs-one (OvO) or one-vs-rest (OvR) strategy. In OvO, a binary classifier is trained for each pair of classes, and the class with the most votes is chosen. In OvR, separate binary classifiers are trained for each class, where each classifier distinguishes between one class and the rest.\n",
    "\n",
    "Random Forest: Ensemble learning method that combines the strengths of decision trees with randomization to achieve high predictive accuracy and generalization performance.\n",
    "\n",
    "Gradient Boosting: Builds an ensemble of weak learners, optimizing them using gradient descent to minimize a loss function and achieve strong predictive performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "def naive_bayes_accuracy_per_class(X, y, n_splits=5, n_repeats=2):\n",
    "    nb_classifier = GaussianNB()\n",
    "\n",
    "    # Perform cross-validation and get predicted labels for each sample\n",
    "    y_pred = cross_val_predict(nb_classifier, X, y, cv=n_splits, n_jobs=-1)\n",
    "\n",
    "    # accuracy for each class\n",
    "    accuracy_per_class = []\n",
    "    for class_label in np.unique(y):\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        class_accuracy = accuracy_score(y[class_indices], y_pred[class_indices])\n",
    "        accuracy_per_class.append(class_accuracy)\n",
    "\n",
    "    return accuracy_per_class\n",
    "\n",
    "# Example:\n",
    "accuracies = naive_bayes_accuracy_per_class(X, y)\n",
    "\n",
    "naive_bayes_results = pd.DataFrame({'Class': range(len(accuracies)), 'Accuracy': accuracies})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def logistic_regression_accuracy_per_class(X, y, n_splits=5, n_repeats=2):\n",
    "    lr_classifier = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "\n",
    "    # cross-validation and get predicted labels for each sample\n",
    "    y_pred = cross_val_predict(lr_classifier, X, y, cv=n_splits, n_jobs=-1)\n",
    "\n",
    "    # accuracy for each class\n",
    "    accuracy_per_class = []\n",
    "    for class_label in np.unique(y):\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        class_accuracy = accuracy_score(y[class_indices], y_pred[class_indices])\n",
    "        accuracy_per_class.append(class_accuracy)\n",
    "\n",
    "    return accuracy_per_class\n",
    "\n",
    "# Example:\n",
    "accuracies = logistic_regression_accuracy_per_class(X, y)\n",
    "\n",
    "logistic_results = pd.DataFrame({'Class': range(len(accuracies)), 'Accuracy': accuracies})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "def svm_accuracy_per_class(X, y, n_splits=5, n_repeats=2):\n",
    "    svm_classifier = SVC()\n",
    "\n",
    "    # cross-validation and get predicted labels for each sample\n",
    "    y_pred = cross_val_predict(svm_classifier, X, y, cv=n_splits, n_jobs=-1)\n",
    "\n",
    "    # accuracy for each class\n",
    "    accuracy_per_class = []\n",
    "    for class_label in np.unique(y):\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        class_accuracy = accuracy_score(y[class_indices], y_pred[class_indices])\n",
    "        accuracy_per_class.append(class_accuracy)\n",
    "\n",
    "    return accuracy_per_class\n",
    "\n",
    "# Example:\n",
    "accuracies = svm_accuracy_per_class(X, y)\n",
    "\n",
    "svm_results = pd.DataFrame({'Class': range(len(accuracies)), 'Accuracy': accuracies})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def random_forest_accuracy_per_class(X, y, n_splits=5, n_repeats=2):\n",
    "    rf_classifier = RandomForestClassifier()\n",
    "\n",
    "    # cross-validation and get predicted labels for each sample\n",
    "    y_pred = cross_val_predict(rf_classifier, X, y, cv=n_splits, n_jobs=-1)\n",
    "\n",
    "    # accuracy for each class\n",
    "    accuracy_per_class = []\n",
    "    for class_label in np.unique(y):\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        class_accuracy = accuracy_score(y[class_indices], y_pred[class_indices])\n",
    "        accuracy_per_class.append(class_accuracy)\n",
    "\n",
    "    return accuracy_per_class\n",
    "\n",
    "# Example:\n",
    "accuracies = random_forest_accuracy_per_class(X, y)\n",
    "\n",
    "random_forest_results = pd.DataFrame({'Class': range(len(accuracies)), 'Accuracy': accuracies})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "def gradient_boosting_accuracy_per_class(X, y, n_splits=5, n_repeats=2):\n",
    "    gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "    # cross-validation and predicted class for each sample\n",
    "    y_pred = cross_val_predict(gb_classifier, X, y, cv=n_splits, n_jobs=-1)\n",
    "\n",
    "    # accuracy for each class\n",
    "    accuracy_per_class = []\n",
    "    for class_label in np.unique(y):\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        class_accuracy = accuracy_score(y[class_indices], y_pred[class_indices])\n",
    "        accuracy_per_class.append(class_accuracy)\n",
    "\n",
    "    return accuracy_per_class\n",
    "\n",
    "# Example:\n",
    "accuracies = gradient_boosting_accuracy_per_class(X, y)\n",
    "\n",
    "gradient_bosting_results = pd.DataFrame({'Class': range(len(accuracies)), 'Accuracy': accuracies})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Class</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.208609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.933233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.008882</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Class  Accuracy\n",
       "0      0  0.208609\n",
       "1      1  0.933233\n",
       "2      2  0.008882"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.concat([naive_bayes_results, logistic_results, svm_results, random_forest_results, gradient_bosting_results], axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chicago",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
